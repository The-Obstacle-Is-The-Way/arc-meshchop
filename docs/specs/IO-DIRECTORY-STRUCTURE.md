# IO Directory Structure Specification

> **Version**: 1.0
> **Status**: Active
> **Last Updated**: 2024-12-14

This document specifies the input/output directory structure for arc-meshchop training experiments.

---

## Quick Reference

```
project_root/
├── data/arc/                     # INPUT: Dataset (from HuggingFace)
│   ├── dataset_info.json         # Metadata (paths, volumes, acquisition types)
│   └── cache/                    # Preprocessed data cache
│       └── outer_{fold}/train/   # Per-fold cache (shared across restarts)
│
├── experiments/                  # OUTPUT: Training results
│   ├── experiment_results.json   # Aggregated results (final output)
│   └── fold_{N}_restart_{M}/     # Per-run directory
│       ├── results.json          # Run-level results
│       └── final.pt              # Model checkpoint
│
└── exports/                      # OUTPUT: Exported models (optional)
    ├── meshnet26.onnx
    └── meshnet26_tfjs/
```

---

## 1. Input Directory: `data/arc/`

### Source
Downloaded via HuggingFace datasets API:
```bash
uv run arc-meshchop download --output data/arc
```

### Structure

```
data/arc/
├── dataset_info.json        # REQUIRED: Dataset metadata
├── cache/                   # GENERATED: Preprocessed tensors
│   ├── outer_0/
│   │   └── train/
│   │       ├── sample_0000_a1b2c3d4.npz
│   │       └── ...
│   ├── outer_1/
│   │   └── train/
│   └── outer_2/
│       └── train/
├── images/                  # OPTIONAL: Local NIfTI copies
│   └── sub-M2001_ses-1_t2w.nii.gz
└── masks/                   # OPTIONAL: Local NIfTI copies
    └── sub-M2001_ses-1_lesion.nii.gz
```

### Key Files

| File | Purpose | Format |
|------|---------|--------|
| `dataset_info.json` | Dataset index | JSON with `image_paths`, `mask_paths`, `lesion_volumes`, `acquisition_types` |
| `cache/outer_{fold}/train/*.npz` | Preprocessed tensors | NumPy compressed (image + mask arrays) |

### Cache Strategy

- **Cache key**: Hash of file path + preprocessing params (prevents collisions)
- **Shared across restarts**: Cache is per outer-fold, NOT per-restart
  - `outer_0/train/` used by all 10 restarts of fold 0
  - Prevents 10x disk usage explosion
- **Generated on first access**: Lazy loading, auto-populated

---

## 2. Output Directory: `experiments/`

### Source
Generated by experiment runner:
```bash
uv run arc-meshchop experiment --output experiments/meshnet26
```

### Structure

```
experiments/
├── experiment_results.json      # FINAL: Aggregated experiment results
└── fold_{outer}_restart_{r}/    # Per-run directory (30 total)
    ├── results.json             # Run-level results + per-subject scores
    ├── final.pt                 # Final model checkpoint (Trainer saves)
    ├── best.pt                  # Best checkpoint (if validation enabled)
    └── epoch_{N}.pt             # Periodic checkpoints (if enabled)
```

### Naming Convention

| Pattern | Example | Description |
|---------|---------|-------------|
| `fold_{outer}_restart_{r}` | `fold_0_restart_5` | Outer fold 0, restart 5 |

**30 total runs**: 3 outer folds × 10 restarts

### Key Output Files

#### `experiment_results.json` (Top-level aggregation)

```json
{
  "config": { ... },
  "folds": [
    {
      "outer_fold": 0,
      "mean_dice": 0.875,
      "std_dice": 0.012,
      "best_dice": 0.891,
      "num_restarts": 10,
      "runs": [ ... ]
    }
  ],
  "summary": {
    "test_mean_dice": 0.876,
    "test_std_dice": 0.016,
    "test_median_dice": 0.881,
    "test_iqr_dice": [0.865, 0.892],
    "test_mean_dice_per_run": 0.874,
    "test_std_dice_per_run": 0.018,
    "test_mean_avd": 0.245,
    "test_std_avd": 0.036,
    "test_mean_mcc": 0.760,
    "test_std_mcc": 0.030,
    "num_test_subjects": 224,
    "target_dice": 0.876,
    "paper_parity": true
  },
  "timing": {
    "start_time": "2024-12-14T10:00:00",
    "end_time": "2024-12-15T14:30:00",
    "total_duration_hours": 28.5
  }
}
```

#### `fold_{N}_restart_{M}/results.json` (Per-run results)

```json
{
  "outer_fold": 0,
  "restart": 5,
  "seed": 47,
  "test_dice": 0.881,
  "test_avd": 0.238,
  "test_mcc": 0.765,
  "final_train_loss": 0.0842,
  "checkpoint_path": "experiments/fold_0_restart_5/final.pt",
  "duration_seconds": 3420.5,
  "per_subject_dice": [0.91, 0.87, ...],
  "per_subject_avd": [0.21, 0.25, ...],
  "per_subject_mcc": [0.82, 0.74, ...],
  "subject_indices": [150, 151, ...]
}
```

#### `fold_{N}_restart_{M}/final.pt` (Checkpoint)

```python
{
    "model_state_dict": {...},
    "optimizer_state_dict": {...},
    "scheduler_state_dict": {...},
    "scaler_state_dict": {...},
    "training_state": {
        "epoch": 50,
        "global_step": 7400,
        "best_dice": 0.881,
        "best_epoch": 47
    },
    "config": {...}
}
```

---

## 3. Export Directory: `exports/` (Optional)

### Source
Generated by export pipeline (Python API, not CLI):
```python
from arc_meshchop.export import export_to_onnx

export_to_onnx(
    checkpoint_path="experiments/fold_0_restart_5/final.pt",
    output_dir="exports/",
)
```

> **Note:** Export is available via `arc_meshchop.export` Python API, not as a CLI command.

### Structure

```
exports/
├── meshnet26.onnx              # ONNX format
├── meshnet26_int8.onnx         # Quantized ONNX (optional)
├── meshnet26_tfjs/             # TensorFlow.js (optional)
│   ├── model.json
│   └── group1-shard1of1.bin
└── model_info.json             # Export metadata
```

---

## 4. CLI Command Reference

### Download Data
```bash
uv run arc-meshchop download \
    --output data/arc           # Output directory
```

**Creates**: `data/arc/dataset_info.json`

### Run Experiment
```bash
uv run arc-meshchop experiment \
    --data-dir data/arc         # Input directory
    --output experiments/run1   # Output directory
    --variant meshnet_26        # Model variant
    --restarts 10               # Restarts per fold
    --epochs 50                 # Epochs per run
```

**Creates**:
- `experiments/run1/experiment_results.json`
- `experiments/run1/fold_{N}_restart_{M}/results.json`
- `experiments/run1/fold_{N}_restart_{M}/final.pt`

### Single Training Run (Debug/HPO)

> **Note:** The `train` command uses nested CV (outer + inner folds) for debugging
> and HPO. The `experiment` command uses outer-only folds per the paper protocol.

```bash
uv run arc-meshchop train \
    --data-dir data/arc         # Input directory
    --output checkpoints/       # Output directory
    --outer-fold 0              # Outer fold
    --inner-fold 0              # Inner fold (for debug/HPO)
    --epochs 10                 # Epochs
```

**Creates**:
- `checkpoints/fold_0_0/final.pt`
- `checkpoints/fold_0_0/results.json`

### Evaluate Checkpoint
```bash
uv run arc-meshchop evaluate \
    checkpoints/fold_0_0/final.pt   # Checkpoint path
    --data-dir data/arc             # Data directory
    --outer-fold 0                  # Fold to evaluate
```

**Output**: Prints metrics to console, optionally saves JSON

### Export Model (Python API)
```python
from arc_meshchop.export import export_to_onnx

export_to_onnx(
    checkpoint_path="experiments/fold_0_restart_5/final.pt",
    output_dir="exports/",
)
```

**Creates**: `exports/meshnet26.onnx`

> **Note:** No CLI `export` command exists. Use the Python API directly.

---

## 5. Resume Capability

### Skip Completed Runs
The experiment runner checks for existing `results.json` files:

```bash
# Resume interrupted experiment
uv run arc-meshchop experiment \
    --data-dir data/arc \
    --output experiments/run1 \
    --skip-completed            # Default: True
```

If `experiments/run1/fold_0_restart_5/results.json` exists, that run is skipped.

### Force Re-run
```bash
uv run arc-meshchop experiment \
    --data-dir data/arc \
    --output experiments/run1 \
    --no-skip-completed         # Force re-run all
```

---

## 6. Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `TORCH_DEVICE` | auto | Override device selection (`cuda`, `mps`, `cpu`) |
| `ARC_DATA_DIR` | `data/arc` | Default data directory |
| `ARC_OUTPUT_DIR` | `experiments` | Default output directory |

---

## 7. Disk Space Estimates

| Component | Size | Notes |
|-----------|------|-------|
| `data/arc/dataset_info.json` | ~100KB | Metadata only (paths to HF cache) |
| `data/arc/cache/` | ~50GB | Preprocessed 256³ volumes (224 samples × ~200MB each) |
| `experiments/fold_*/` | ~5MB each | Checkpoint (~2MB) + results (~3MB with per-subject) |
| `experiments/` total | ~150MB | 30 runs |
| `exports/meshnet26.onnx` | ~600KB | Exported model |

**Note**: Cache is shared across restarts. Without this optimization, it would be 10× larger.

---

## 8. Troubleshooting

### "Dataset info not found"
```
FileNotFoundError: Dataset info not found: data/arc/dataset_info.json
```
**Fix**: Run `uv run arc-meshchop download --output data/arc` first.

### "Checkpoint not found"
```
FileNotFoundError: No such file: experiments/fold_0_restart_0/final.pt
```
**Fix**: Training did not complete. Check logs and re-run.

### Stale Cache
If preprocessing changes, delete the cache:
```bash
rm -rf data/arc/cache/
```

### Disk Full
Cache is the largest component. Clear it if needed:
```bash
du -sh data/arc/cache/   # Check size
rm -rf data/arc/cache/   # Clear (will regenerate)
```
